{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer, Reshape, LSTM, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from keras_tuner import RandomSearch, BayesianOptimization\n",
    "from tensorflow.keras.optimizers.legacy import Adam, Adagrad, Adadelta, SGD,RMSprop\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 224\n",
    "img_width = 224\n",
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9098fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    validation_split=0.05,\n",
    "    subset='training',\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "ts = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "     data_dir,\n",
    "     seed=123,\n",
    "     image_size=(img_height,img_width),\n",
    "     validation_split=0.05,\n",
    "     subset=\"validation\",\n",
    "     shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419061a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor=\"val_loss\",patience=7, verbose=1)\n",
    "\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data,training_data):\n",
    "        super(MetricsCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.training_data = training_data\n",
    "        self.true_labels = []\n",
    "        self.predicted_labels = []\n",
    "        self.true_labels1 = []\n",
    "        self.predicted_labels1 = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Training metrics\n",
    "        for images, labels in self.training_data:\n",
    "            self.true_labels1.extend(labels.numpy())\n",
    "            predictions = model.predict(images, verbose=0)\n",
    "            self.predicted_labels1.extend(tf.argmax(predictions, axis=1).numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_precision = precision_score(self.true_labels1, self.predicted_labels1, average='weighted')\n",
    "        train_recall = recall_score(self.true_labels1, self.predicted_labels1, average='weighted')\n",
    "        train_f1 = f1_score(self.true_labels1, self.predicted_labels1, average='weighted')\n",
    "\n",
    "#         print(f'Epoch {epoch + 1} - Validation Metrics: F1 Score: {val_f1}, Precision: {val_precision}, Recall: {val_recall}')\n",
    "        # Validation metrics\n",
    "        \n",
    "        for images, labels in self.validation_data:\n",
    "            self.true_labels.extend(labels.numpy())\n",
    "            predictions = model.predict(images, verbose=0)\n",
    "            self.predicted_labels.extend(tf.argmax(predictions, axis=1).numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        val_precision = precision_score(self.true_labels, self.predicted_labels, average='weighted')\n",
    "        val_recall = recall_score(self.true_labels, self.predicted_labels, average='weighted')\n",
    "        val_f1 = f1_score(self.true_labels, self.predicted_labels, average='weighted')\n",
    "\n",
    "        print(f'  Train_F1 Score: {train_f1},Val_F1 Score: {val_f1} , Train_Precision: {train_precision} , Val_Precision: {val_precision}, Train_Recall: {train_recall}, Val_Recall: {val_recall}')\n",
    "\n",
    "# Example usage during training\n",
    "callback = MetricsCallback(validation_data=ts,training_data=tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da7f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    \n",
    "    effnet = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(img_height,img_width,3))\n",
    "    output = effnet.layers[-1].output\n",
    "    output = tf.keras.layers.Flatten()(output)\n",
    "    output = Reshape((1, -1))(output)\n",
    "    effnet = Model(effnet.input, output)\n",
    "    for layer in effnet.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model1 = Sequential()\n",
    "    model1.add(effnet)\n",
    "    \n",
    "    unit_params = hp.Choice('units', values = [512, 1024, 2048])\n",
    "    dropout_params = hp.Choice('dropouts', values = [0.2, 0.0])\n",
    "    optimizer_params = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop','Adadelta', 'Adagrad'])\n",
    "    learning_rate_params = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "    \n",
    "    model1.add(LSTM(units=128, return_sequences=False, input_shape=(1, base_model.output_shape[1])))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(Dense(units = unit_params, activation='relu', input_dim=(img_height,img_width,3)))\n",
    "    model1.add(tf.keras.layers.Dropout(0.2))\n",
    "    model1.add(Dense(7, activation=\"softmax\"))\n",
    "    \n",
    "    \n",
    "    if optimizer_params == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate_params)\n",
    "    elif optimizer_params == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate_params)\n",
    "    elif optimizer_params == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate_params)\n",
    "    elif optimizer_params == 'Adagrad':\n",
    "        optimizer = Adagrad(learning_rate=learning_rate_params)\n",
    "    elif optimizer_params == 'Adadelta':\n",
    "        optimizer = Adadelta(learning_rate=learning_rate_params)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid optimizer choice: {optimizer_choice}\")\n",
    "\n",
    "    \n",
    "    model1.compile(optimizer = optimizer, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f1feac",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e903f85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuner = RandomSearch(build_model, objective= kt.Objective(\"val_accuracy\", direction=\"max\"), max_trials = 15 , executions_per_trial=1, directory='logs/Randomsearch/effNet_LSTM', project_name='effNet_LSTM_tuned')\n",
    "tuner.search(tr, epochs = 5, \n",
    "             validation_data=ts, \n",
    "             callbacks = [early_stopping,tf.keras.callbacks.TensorBoard(\"logs/Randomsearch/effNet_LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b8de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a10955",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c75cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5273befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(tr, epochs=30, validation_data = ts, callbacks = [early_stopping, callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47607d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d8894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ts.class_names\n",
    "\n",
    "# Get true labels and model predictions\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for images, labels in ts:\n",
    "    true_labels.extend(labels.numpy())\n",
    "    predictions = model.predict(images)\n",
    "    predicted_labels.extend(tf.argmax(predictions, axis=1).numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    \n",
    "# # Find misclassified samples\n",
    "# misclassified_indices = [i for i, (true, pred) in enumerate(zip(true_labels, predicted_labels)) if true != pred]\n",
    "# misclassified_samples = [(class_names[true], class_names[pred]) for true, pred in zip(true_labels, predicted_labels) if true != pred]\n",
    "\n",
    "# # Print misclassifications\n",
    "# for sample in misclassified_samples:\n",
    "#     print(f\"True: {sample[0]}, Predicted: {sample[1]}\")\n",
    "\n",
    "# # Calculate overall accuracy\n",
    "# accuracy = 1 - (len(misclassified_indices) / len(true_labels))\n",
    "# print(f\"Overall accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42823d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb364e40",
   "metadata": {},
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e411ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor=\"val_loss\",patience=7, verbose=1)\n",
    "\n",
    "tuner = RandomSearch(build_model, objective= kt.Objective(\"val_accuracy\", direction=\"max\"), max_trials = 30 , executions_per_trial=1, directory='logs/Bayesiansearch/effNet_LSTM', project_name='effNet_tuned_LSTM')\n",
    "tuner.search(tr, epochs = 5, \n",
    "             validation_data=ts, \n",
    "             callbacks = [early_stopping,tf.keras.callbacks.TensorBoard(\"logs/Bayesiansearch/effNet_LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b62494",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ca8ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(tr, epochs=30, validation_data = ts, callbacks = [early_stopping, callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992fb95d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e54ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ts.class_names\n",
    "\n",
    "# Get true labels and model predictions\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for images, labels in ts:\n",
    "    true_labels.extend(labels.numpy())\n",
    "    predictions = model.predict(images)\n",
    "    predicted_labels.extend(tf.argmax(predictions, axis=1).numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    \n",
    "# # Find misclassified samples\n",
    "# misclassified_indices = [i for i, (true, pred) in enumerate(zip(true_labels, predicted_labels)) if true != pred]\n",
    "# misclassified_samples = [(class_names[true], class_names[pred]) for true, pred in zip(true_labels, predicted_labels) if true != pred]\n",
    "\n",
    "# # Print misclassifications\n",
    "# for sample in misclassified_samples:\n",
    "#     print(f\"True: {sample[0]}, Predicted: {sample[1]}\")\n",
    "\n",
    "# # Calculate overall accuracy\n",
    "# accuracy = 1 - (len(misclassified_indices) / len(true_labels))\n",
    "# print(f\"Overall accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70f022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998eda15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
